{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72cb527e",
   "metadata": {},
   "source": [
    "# Introductory cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b5254f",
   "metadata": {},
   "source": [
    " first we load the scaled data as saved in the preprocessing and engineering notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9146ac3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eseoghene\\AppData\\Local\\Temp\\ipykernel_5896\\1762555031.py:6: FutureWarning: The squeeze argument has been deprecated and will be removed in a future version. Append .squeeze(\"columns\") to the call to squeeze.\n",
      "\n",
      "\n",
      "  y_train = pd.read_csv(\"../data_details/y_train.csv\", header=None, squeeze=True)  # squeeze=True converts it to a Series\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1460, 1461]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m y_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data_details/y_train.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, squeeze\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# squeeze=True converts it to a Series\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Split the data into training and validation sets (80% train, 20% validation)\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m X_train, X_val, y_train_split, y_val \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m X_train\u001b[38;5;241m.\u001b[39mshape, X_val\u001b[38;5;241m.\u001b[39mshape, y_train_split\u001b[38;5;241m.\u001b[39mshape, y_val\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\house-price-prediction-Pfy1jhir-py3.11\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\house-price-prediction-Pfy1jhir-py3.11\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2614\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2611\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_arrays \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2612\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2614\u001b[0m arrays \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2616\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   2617\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2618\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m   2619\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\house-price-prediction-Pfy1jhir-py3.11\\Lib\\site-packages\\sklearn\\utils\\validation.py:455\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \n\u001b[0;32m    438\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    454\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[1;32m--> 455\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\house-price-prediction-Pfy1jhir-py3.11\\Lib\\site-packages\\sklearn\\utils\\validation.py:409\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    407\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 409\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    411\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    412\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1460, 1461]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data\n",
    "X_train_scaled = pd.read_csv(\"../data_details/train_data_scaled.csv\")\n",
    "y_train = pd.read_csv(\"../data_details/y_train.csv\", header=None, squeeze=True)  # squeeze=True converts it to a Series\n",
    "\n",
    "# Split the data into training and validation sets (80% train, 20% validation)\n",
    "X_train, X_val, y_train_split, y_val = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.shape, X_val.shape, y_train_split.shape, y_val.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c686cd4",
   "metadata": {},
   "source": [
    "there is an error because there is a mismatch between the number of samples in the feature set (X_train_scaled) and the target variable (y_train). The feature set has 1460 samples, while the target variable has 1461.\n",
    "\n",
    " I'll check the first few rows of each dataset to see if we can identify the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59247ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(         Id  MSSubClass  LotFrontage   LotArea  OverallQual  OverallCond  \\\n",
       " 0 -1.730865    0.073375    -0.231877 -0.207142     0.651479    -0.517200   \n",
       " 1 -1.728492   -0.872563     0.437043 -0.091886    -0.071836     2.179628   \n",
       " 2 -1.726120    0.073375    -0.098093  0.073480     0.651479    -0.517200   \n",
       " 3 -1.723747    0.309859    -0.454850 -0.096897     0.651479    -0.517200   \n",
       " 4 -1.721374    0.073375     0.615421  0.375148     1.374795    -0.517200   \n",
       " \n",
       "    YearBuilt  YearRemodAdd  MasVnrArea  BsmtFinSF1  ...  SaleCondition_Family  \\\n",
       " 0   1.050994      0.878668    0.514104    0.575425  ...             -0.117851   \n",
       " 1   0.156734     -0.429577   -0.570750    1.171992  ...             -0.117851   \n",
       " 2   0.984752      0.830215    0.325915    0.092907  ...             -0.117851   \n",
       " 3  -1.863632     -0.720298   -0.570750   -0.499274  ...             -0.117851   \n",
       " 4   0.951632      0.733308    1.366489    0.463568  ...             -0.117851   \n",
       " \n",
       "    SaleCondition_Normal  SaleCondition_Partial  TotalArea  AgeAtSale  \\\n",
       " 0              0.467651              -0.305995  -0.150528  -1.043259   \n",
       " 1              0.467651              -0.305995  -0.067924  -0.183465   \n",
       " 2              0.467651              -0.305995   0.111099  -0.977121   \n",
       " 3             -2.138345              -0.305995  -0.088178   1.800676   \n",
       " 4              0.467651              -0.305995   0.566402  -0.944052   \n",
       " \n",
       "    TotalBath  TotalPorchSF   HasPool  Remodeled  OverallQual_GrLivArea  \n",
       " 0   1.642256     -0.768375 -0.069409  -0.954460               0.442827  \n",
       " 1   0.368581      0.745011 -0.069409  -0.954460              -0.405394  \n",
       " 2   1.642256     -0.889702 -0.069409   1.047712               0.545431  \n",
       " 3  -0.268257      0.802481 -0.069409   1.047712               0.452277  \n",
       " 4   1.642256      0.604528 -0.069409  -0.954460               1.525572  \n",
       " \n",
       " [5 rows x 267 columns],\n",
       " 0    SalePrice\n",
       " 1       208500\n",
       " 2       181500\n",
       " 3       223500\n",
       " 4       140000\n",
       " Name: 0, dtype: object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the first few rows of X_train_scaled and y_train to inspect the data\n",
    "X_train_scaled.head(), y_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab33b099",
   "metadata": {},
   "source": [
    "so it appears that the y_train dataset has an additional header row with the label \"SalePrice.\" i think this is what is causing the mismatch in the number of samples.\n",
    "\n",
    "I'll remove this header row from y_train and then check if it works \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "389c80bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1168, 267), (292, 267), (1168,), (292,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing the header row from y_train\n",
    "y_train = y_train.iloc[1:].astype(int)\n",
    "\n",
    "# Split the data into training and validation sets again\n",
    "X_train, X_val, y_train_split, y_val = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.shape, X_val.shape, y_train_split.shape, y_val.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a66e38",
   "metadata": {},
   "source": [
    "The data is now properly split into training and validation sets. We have 1168 samples in the training set and 292 samples in the validation set.\n",
    "\n",
    "Next we can move on to training our regression models:\n",
    "\n",
    "- Linear Regression\n",
    "- Random Forest\n",
    "- Gradient Boosting (using XGBoost)\n",
    "\n",
    "\n",
    "For each model, I plan to:\n",
    "\n",
    "Train the model using the training set.\n",
    "Evaluate the model using the validation set.\n",
    "Record the Root Mean Squared Error (RMSE) for each model as it's a common metric for regression problems and it's particularly relevant for our dataset since the Kaggle competition also uses RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc1e3bc",
   "metadata": {},
   "source": [
    "first linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3166285a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1093041192553732.4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Initialize and train the Linear Regression model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train_split)\n",
    "\n",
    "# Predict on the validation set\n",
    "lr_val_predictions = lr_model.predict(X_val)\n",
    "\n",
    "# Calculate the RMSE for Linear Regression\n",
    "lr_rmse = np.sqrt(mean_squared_error(y_val, lr_val_predictions))\n",
    "lr_rmse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb366b73",
   "metadata": {},
   "source": [
    "then random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0393ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30447.655239822787"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train_split)\n",
    "\n",
    "# Predict on the validation set\n",
    "rf_val_predictions = rf_model.predict(X_val)\n",
    "\n",
    "# Calculate the RMSE for Random Forest\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_val, rf_val_predictions))\n",
    "rf_rmse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ec7c9a",
   "metadata": {},
   "source": [
    "then gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "435a9090",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Initialize and train the XGBoost model\u001b[39;00m\n\u001b[0;32m      4\u001b[0m xgb_model \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.08\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, subsample\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.75\u001b[39m, colsample_bytree\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Initialize and train the XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75, colsample_bytree=1, max_depth=7, random_state=42)\n",
    "xgb_model.fit(X_train, y_train_split)\n",
    "\n",
    "# Predict on the validation set\n",
    "xgb_val_predictions = xgb_model.predict(X_val)\n",
    "\n",
    "# Calculate the RMSE for XGBoost\n",
    "xgb_rmse = np.sqrt(mean_squared_error(y_val, xgb_val_predictions))\n",
    "xgb_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00347a98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
