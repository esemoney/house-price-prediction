{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08b5254f",
   "metadata": {},
   "source": [
    "# Model Building, Training and Evaluation\n",
    "\n",
    "In this notebook, we will be building, training, and evaluating machine learning models for our housing prices prediction task.\n",
    "\n",
    "It is a logical continuation of our end-to-end ML workflow and extends the workflow from our [previous notebook](2_data_cleaning_and_processing_and_feature_engineering.ipynb). We have performed several preprocessing and feature engineering steps to prepare our dataset for modeling. Here's a summary of what we've done so far:\n",
    "\n",
    "- Data Cleaning: We handled missing values in the dataset. For some features, NaN values were replaced with 'None' or 0, depending on the feature's description in the data dictionary. For others, we used the mode or median value of the feature.\n",
    "\n",
    "- Outlier Removal: We identified potential outliers in the dataset using scatter plots and removed them based on certain criteria. This step is crucial to prevent our models from being unduly influenced by extreme values.\n",
    "\n",
    "- Categorical Feature Encoding: We converted categorical variables into a format that can be provided to a machine learning algorithm. We used ordinal encoding for ordinal features and one-hot encoding for nominal features.\n",
    "\n",
    "- Feature Engineering: We created new features based on domain knowledge and exploratory data analysis. These new features can help improve the performance of our models by introducing new information or highlighting certain patterns.\n",
    "\n",
    "- Normalization: We applied a log transformation to the numerical features in the dataset to stabilize the variance, reduce skewness, and diminish the influence of outliers.\n",
    "\n",
    "- Feature Scaling: We standardized the features in the dataset to have a mean of 0 and a standard deviation of 1. This step is important for many machine learning algorithms that are sensitive to the scale of the features.\n",
    "\n",
    "- Feature Selection: We calculated Mutual Information (MI) scores for the features and identified those with an MI score of 0. These features do not contribute any predictive power to our models and were dropped from the dataset.\n",
    "\n",
    "We have saved different versions of the dataset after each major step. This allows us to experiment with different combinations of preprocessing and feature engineering steps and see which one produces the best results. We have three main paths for our modeling phase:\n",
    "\n",
    "- Path 1 - Model with All Features: In this path, we will use the dataset that includes all features (original and engineered) without any feature selection. This will serve as our baseline.\n",
    "\n",
    "- Path 2 - Model with Zero MI Features Dropped: In this path, we will use the dataset where features with an MI score of 0 have been dropped. This allows us to see if removing these non-informative features improves the performance of our models.\n",
    "\n",
    "- Path 3 - Further Feature Selection and Dimensionality Reduction: In this path, we can explore other feature selection techniques and dimensionality reduction methods, such as Principal Component Analysis (PCA). This can help us reduce the dimensionality of our dataset and potentially improve the performance and interpretability of our models.\n",
    "\n",
    "In this notebook, we will train different models mainly 3 regressin models so maybe linear regression, ridge or lasso regression, xg boost, and a neural network on these datasets and evaluate their performance. This will help us understand the impact of our preprocessing and feature engineering decisions and guide us in improving our models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efc81e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e3c4167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MSSubClass  LotFrontage   LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
      "0    0.425401    -0.076972 -0.119598     0.694385    -0.460159   1.045693   \n",
      "1   -1.124498     0.561642  0.139916     0.031855     1.944960   0.164918   \n",
      "2    0.425401     0.061643  0.462502     0.694385    -0.460159   0.980858   \n",
      "3    0.646045    -0.322636  0.129296     0.694385    -0.460159  -1.870098   \n",
      "4    0.425401     0.711952  0.944727     1.278778    -0.460159   0.948417   \n",
      "\n",
      "   YearRemodAdd  MasVnrArea  ExterQual  ExterCond  ...  SaleCondition_Normal  \\\n",
      "0      0.879312    1.211932   1.060664  -0.238718  ...              0.466838   \n",
      "1     -0.422181   -0.804570  -0.688649  -0.238718  ...              0.466838   \n",
      "2      0.831422    1.139621   1.060664  -0.238718  ...              0.466838   \n",
      "3     -0.713815   -0.804570  -0.688649  -0.238718  ...             -2.142069   \n",
      "4      0.735570    1.432385   1.060664  -0.238718  ...              0.466838   \n",
      "\n",
      "   SaleCondition_Partial  TotalArea   TotalSF  AgeAtSale  TotalBath  \\\n",
      "0              -0.304107   0.852589  1.088777  -1.038232   1.800266   \n",
      "1              -0.304107  -0.282874 -0.693617  -0.191033   0.202444   \n",
      "2              -0.304107   0.813204  1.135900  -0.973493   1.800266   \n",
      "3              -0.304107   0.707101  1.049929   1.808693   0.066689   \n",
      "4              -0.304107   1.795895  1.324551  -0.941099   1.800266   \n",
      "\n",
      "   TotalPorchSF   HasPool  Remodeled  OverallQual_GrLivArea  \n",
      "0     -0.484753 -0.064393  -0.953588               0.707110  \n",
      "1     -0.067214 -0.064393  -0.953588              -0.123500  \n",
      "2     -0.581868 -0.064393   1.048671               0.754473  \n",
      "3      0.859671 -0.064393   1.048671               0.711560  \n",
      "4      0.995643 -0.064393  -0.953588               1.455616  \n",
      "\n",
      "[5 rows x 212 columns]\n",
      "   MSSubClass  LotFrontage   LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
      "0   -1.124498     0.561642  0.528669    -0.732980     0.454225  -0.329614   \n",
      "1   -1.124498     0.599904  0.945725     0.031855     0.454225  -0.428974   \n",
      "2    0.425401     0.321653  0.882451    -0.732980    -0.460159   0.850995   \n",
      "3    0.425401     0.483680  0.218464     0.031855     0.454225   0.883485   \n",
      "4    1.420900    -1.341341 -1.184744     1.278778    -0.460159   0.688299   \n",
      "\n",
      "   YearRemodAdd  MasVnrArea  ExterQual  ExterCond  ...  SaleCondition_Normal  \\\n",
      "0     -1.152935   -0.804570  -0.688649  -0.238718  ...              0.466838   \n",
      "1     -1.299756    0.986032  -0.688649  -0.238718  ...              0.466838   \n",
      "2      0.639623   -0.804570  -0.688649  -0.238718  ...              0.466838   \n",
      "3      0.639623    0.357469  -0.688649  -0.238718  ...              0.466838   \n",
      "4      0.351203   -0.804570   1.060664  -0.238718  ...              0.466838   \n",
      "\n",
      "   SaleCondition_Partial  TotalArea   TotalSF  AgeAtSale  TotalBath  \\\n",
      "0              -0.304107   0.640262 -0.902448   0.399566  -1.531133   \n",
      "1              -0.304107   0.732594 -0.662781   0.498779  -0.732222   \n",
      "2              -0.304107   0.840902  1.078090  -0.779307   0.202444   \n",
      "3              -0.304107   1.185889  1.066868  -0.811749   0.202444   \n",
      "4              -0.304107  -0.064111 -0.685175  -0.616851  -0.596467   \n",
      "\n",
      "   TotalPorchSF   HasPool  Remodeled  OverallQual_GrLivArea  \n",
      "0      1.006048 -0.064393  -0.953588              -1.021756  \n",
      "1      0.964308 -0.064393  -0.953588              -0.070786  \n",
      "2      0.786330 -0.064393   1.048671              -0.460889  \n",
      "3      0.941093 -0.064393  -0.953588               0.120885  \n",
      "4      0.913434 -0.064393  -0.953588               0.833371  \n",
      "\n",
      "[5 rows x 212 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the scaled train and test datasets\n",
    "scaled_train_data_path = '../data_details/train_data_scaled.csv'\n",
    "scaled_test_data_path = '../data_details/test_data_scaled.csv'\n",
    "\n",
    "scaled_train_data = pd.read_csv(scaled_train_data_path)\n",
    "scaled_test_data = pd.read_csv(scaled_test_data_path)\n",
    "\n",
    "# Display the first few rows of the scaled train dataset\n",
    "print(scaled_train_data.head())\n",
    "\n",
    "# Display the first few rows of the scaled test dataset\n",
    "print(scaled_test_data.head())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "512b309b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RMSE: 32136804843.254498\n",
      "Ridge Regression RMSE: 0.12130846072149407\n",
      "Lasso Regression RMSE: 0.41734378753081575\n",
      "XGBoost Regression RMSE: 0.13091274763742594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eseoghene\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\house-price-prediction-Pfy1jhir-py3.11\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1625: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network RMSE: 2.4241068400523513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eseoghene\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\house-price-prediction-Pfy1jhir-py3.11\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Path 1 - Model with All Features\n",
    "\n",
    "# Split the data into features and target variable\n",
    "data_path = '../data_details/train_data_scaled.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "y_train_path = '../data_details/target_variable.csv'\n",
    "y = pd.read_csv(y_train_path)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(data, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_test = scaled_test_data\n",
    "\n",
    "# Initialize the models\n",
    "linear_regression = LinearRegression()\n",
    "ridge_regression = Ridge()\n",
    "lasso_regression = Lasso()\n",
    "xgboost_regression = xgb.XGBRegressor(objective ='reg:squarederror')\n",
    "\n",
    "# Train the models\n",
    "linear_regression.fit(X_train, y_train)\n",
    "ridge_regression.fit(X_train, y_train)\n",
    "lasso_regression.fit(X_train, y_train)\n",
    "xgboost_regression.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "lr_predictions = linear_regression.predict(X_val)\n",
    "rr_predictions = ridge_regression.predict(X_val)\n",
    "lasso_predictions = lasso_regression.predict(X_val)\n",
    "xgboost_predictions = xgboost_regression.predict(X_val)\n",
    "\n",
    "# Evaluate the models\n",
    "lr_rmse = np.sqrt(mean_squared_error(y_val, lr_predictions))\n",
    "rr_rmse = np.sqrt(mean_squared_error(y_val, rr_predictions))\n",
    "lasso_rmse = np.sqrt(mean_squared_error(y_val, lasso_predictions))\n",
    "xgboost_rmse = np.sqrt(mean_squared_error(y_val, xgboost_predictions))\n",
    "\n",
    "print(f'Linear Regression RMSE: {lr_rmse}')\n",
    "print(f'Ridge Regression RMSE: {rr_rmse}')\n",
    "print(f'Lasso Regression RMSE: {lasso_rmse}')\n",
    "print(f'XGBoost Regression RMSE: {xgboost_rmse}')\n",
    "\n",
    "# Path 2 - Neural Network Model\n",
    "\n",
    "# Initialize the model\n",
    "neural_network = MLPRegressor()\n",
    "\n",
    "# Train the model\n",
    "neural_network.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "nn_predictions = neural_network.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "nn_rmse = np.sqrt(mean_squared_error(y_val, nn_predictions))\n",
    "\n",
    "print(f'Neural Network RMSE: {nn_rmse}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d80e81",
   "metadata": {},
   "source": [
    "## Updating the Neural Network Model in Direction 1\n",
    "\n",
    "We received a warning during the training of the Neural Network model indicating that the maximum number of iterations was reached before convergence. This suggests that the model might need more iterations to find the optimal parameters. Let's increase the `max_iter` parameter and adjust the `learning_rate_init` to see if this improves the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ced00b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eseoghene\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\house-price-prediction-Pfy1jhir-py3.11\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1625: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Neural Network RMSE: 2.1244147160438627\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model with increased max_iter and adjusted learning_rate_init\n",
    "neural_network = MLPRegressor(max_iter=500, learning_rate_init=0.001)\n",
    "\n",
    "# Train the model\n",
    "neural_network.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "nn_predictions = neural_network.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "nn_rmse = np.sqrt(mean_squared_error(y_val, nn_predictions))\n",
    "\n",
    "print(f'Updated Neural Network RMSE: {nn_rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d86d87",
   "metadata": {},
   "source": [
    "## Summary of Direction 1 Results\n",
    "\n",
    "In Direction 1, we trained our models using all available features. Here are the RMSE values we obtained:\n",
    "\n",
    "- Linear Regression RMSE: 32136804843.254498\n",
    "- Ridge Regression RMSE: 0.12130846072149407\n",
    "- Lasso Regression RMSE: 0.41734378753081575\n",
    "- XGBoost Regression RMSE: 0.13091274763742594\n",
    "- Neural Network RMSE: 2.999916166728695\n",
    "\n",
    "The Linear Regression model's RMSE is significantly higher than the others, suggesting that it may not be the best model for this task. The Ridge Regression and XGBoost Regression models have the lowest RMSEs, indicating that they performed the best on our validation set. This is expected as Ridge has inbuilt regularization features. The Lasso Regression model's RMSE is higher than Ridge and XGBoost, indicating it's not performing as well as them. The Neural Network model's RMSE is quite high, suggesting that it's not performing well and might need more tuning, like adjusting the architecture (number of layers, neurons) or the learning rate.\n",
    "\n",
    "\n",
    "## Observations on Neural Network Model Performance\n",
    "\n",
    "After running the Neural Network model again with increased `max_iter` and adjusted `learning_rate_init`, the difference in performance was not significant. The RMSE before the adjustment was 2.4241068400523513, and after the adjustment, it was 2.1244147160438627. This suggests that the adjustments made to the model parameters did not significantly improve the model's performance. It's possible that further tuning of the model parameters or adjustments to the model architecture may be required to achieve better performance.\n",
    "\n",
    "\n",
    "These results provide a good baseline for comparison as we proceed to Direction 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d7e463",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Preparing for Direction 2\n",
    "\n",
    "In Direction 2, we will use the dataset where features with an MI score of 0 have been dropped. This will allow us to see if removing these non-informative features improves the performance of our models.\n",
    "\n",
    "To do this, we will need to load the dataset with zero MI features dropped, then repeat the model training, prediction, and evaluation steps. We will compare the RMSE values obtained in Direction 2 with those from Direction 1 to see if the performance has improved.\n",
    "\n",
    "Let's proceed to Direction 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ad40f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RMSE: 7282268088.739932\n",
      "Ridge Regression RMSE: 0.1223206754782862\n",
      "Lasso Regression RMSE: 0.41734378753081575\n",
      "XGBoost Regression RMSE: 0.1336104636520608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eseoghene\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\house-price-prediction-Pfy1jhir-py3.11\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1625: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network RMSE: 2.2308262104205774\n",
      "In Direction 2, we trained our models using the dataset where features with an MI score of 0 have been dropped. Here are the RMSE values we obtained:\n",
      "Linear Regression RMSE: 7282268088.739932\n",
      "Ridge Regression RMSE: 0.1223206754782862\n",
      "Lasso Regression RMSE: 0.41734378753081575\n",
      "XGBoost Regression RMSE: 0.1336104636520608\n",
      "Neural Network RMSE: 2.2308262104205774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eseoghene\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\house-price-prediction-Pfy1jhir-py3.11\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Path 2 - Model with Zero MI Features Dropped\n",
    "\n",
    "# Load the dataset where features with an MI score of 0 have been dropped\n",
    "data_path = '../data_details/train_data_scaled_dropped.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Load the target variable\n",
    "y_train_path = '../data_details/target_variable.csv'\n",
    "y = pd.read_csv(y_train_path)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(data, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the models\n",
    "linear_regression = LinearRegression()\n",
    "ridge_regression = Ridge()\n",
    "lasso_regression = Lasso()\n",
    "xgboost_regression = xgb.XGBRegressor(objective ='reg:squarederror')\n",
    "\n",
    "# Train the models\n",
    "linear_regression.fit(X_train, y_train)\n",
    "ridge_regression.fit(X_train, y_train)\n",
    "lasso_regression.fit(X_train, y_train)\n",
    "xgboost_regression.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "lr_predictions = linear_regression.predict(X_val)\n",
    "rr_predictions = ridge_regression.predict(X_val)\n",
    "lasso_predictions = lasso_regression.predict(X_val)\n",
    "xgboost_predictions = xgboost_regression.predict(X_val)\n",
    "\n",
    "# Evaluate the models\n",
    "lr_rmse = np.sqrt(mean_squared_error(y_val, lr_predictions))\n",
    "rr_rmse = np.sqrt(mean_squared_error(y_val, rr_predictions))\n",
    "lasso_rmse = np.sqrt(mean_squared_error(y_val, lasso_predictions))\n",
    "xgboost_rmse = np.sqrt(mean_squared_error(y_val, xgboost_predictions))\n",
    "\n",
    "print(f'Linear Regression RMSE: {lr_rmse}')\n",
    "print(f'Ridge Regression RMSE: {rr_rmse}')\n",
    "print(f'Lasso Regression RMSE: {lasso_rmse}')\n",
    "print(f'XGBoost Regression RMSE: {xgboost_rmse}')\n",
    "\n",
    "# Path 2 - Neural Network Model\n",
    "\n",
    "# Initialize the model\n",
    "neural_network = MLPRegressor()\n",
    "\n",
    "# Train the model\n",
    "neural_network.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "nn_predictions = neural_network.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "nn_rmse = np.sqrt(mean_squared_error(y_val, nn_predictions))\n",
    "\n",
    "print(f'Neural Network RMSE: {nn_rmse}')\n",
    "\n",
    "# Summary of Direction 2 Results\n",
    "\n",
    "print(\"In Direction 2, we trained our models using the dataset where features with an MI score of 0 have been dropped. Here are the RMSE values we obtained:\")\n",
    "print(f'Linear Regression RMSE: {lr_rmse}')\n",
    "print(f'Ridge Regression RMSE: {rr_rmse}')\n",
    "print(f'Lasso Regression RMSE: {lasso_rmse}')\n",
    "print(f'XGBoost Regression RMSE: {xgboost_rmse}')\n",
    "print(f'Neural Network RMSE: {nn_rmse}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5af0bb",
   "metadata": {},
   "source": [
    "## Updating the Neural Network Model in Direction 2\n",
    "\n",
    "We received a warning during the training of the Neural Network model indicating that the maximum number of iterations was reached before convergence. This suggests that the model might need more iterations to find the optimal parameters. Let's increase the `max_iter` parameter and adjust the `learning_rate_init` to see if this improves the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0624aa08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eseoghene\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\house-price-prediction-Pfy1jhir-py3.11\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1625: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Neural Network RMSE: 2.1544314492905117\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model with increased max_iter and adjusted learning_rate_init\n",
    "neural_network = MLPRegressor(max_iter=500, learning_rate_init=0.001)\n",
    "\n",
    "# Train the model\n",
    "neural_network.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "nn_predictions = neural_network.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "nn_rmse = np.sqrt(mean_squared_error(y_val, nn_predictions))\n",
    "\n",
    "print(f'Updated Neural Network RMSE: {nn_rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa329dc",
   "metadata": {},
   "source": [
    "\n",
    "## Observations on Neural Network Model Performance\n",
    "\n",
    "After running the Neural Network model again with increased `max_iter` and adjusted `learning_rate_init`, the difference in performance was not significant. The RMSE before the adjustment was 2.2308262104205774, and after the adjustment, it was 2.1544314492905117. This suggests that the adjustments made to the model parameters did not significantly improve the model's performance. It's possible that further tuning of the model parameters or adjustments to the model architecture may be required to achieve better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892950c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
